---
title: Recent Publications
classes: wide
---
<style>
.iconDetails {
	clear: left;
	float:left; 
	width:20%;
    	height:20%;
	max-height:140px;
	max-width:140px; 
} 

.container {
    width:100%;
    height:24%;
    padding:1%;
	margin-bottom: 20px;
}
h4 {
    margin:0px;
}

.button {
    clear: left;
    background-color: #4CAF50; /* Green */
    border: none;
    color: white;
    padding: 4px 20px;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    font-size: 12px;
    margin: 4px 2px;
    -webkit-transition-duration: 0.4s; /* Safari */
    transition-duration: 0.4s;
    cursor: pointer;
}

.green {
    background-color: white; 
    color: black; 
    border: 2px solid #4CAF50;
}

.green:hover {
    background-color: #4CAF50;
    color: white;
}

.blue {
    background-color: white; 
    color: black; 
    border: 2px solid #008CBA;
}

.blue:hover {
    background-color: #008CBA;
    color: white;
}

.red {
    background-color: white; 
    color: black; 
    border: 2px solid #f44336;
}

.red:hover {
    background-color: #f44336;
    color: white;
}

.gray {
    background-color: white;
    color: black;
    border: 2px solid #e7e7e7;
}

.gray:hover {background-color: #e7e7e7;}

.black {
    background-color: white;
    color: black;
    border: 2px solid #555555;
}

.black:hover {
    background-color: #555555;
    color: white;
}
</style>

<script>
function toggleAbstract(btn) {
    var x = btn.nextElementSibling.nextElementSibling;
    if (x.innerHTML === "") {
        x.innerHTML = x.getAttribute("text");
    } else {
        x.innerHTML = "";
    }
}
</script>


My [Google Scholar](https://scholar.google.se/citations?hl=en&user=HgOAYUAAAAAJ) profile.

## References

**2024:**

- Lai, W., Gao, Y., & Lam, T. L. (2024). Vision-Language Model-based Physical Reasoning for Robot Liquid Perception. arXiv Preprint arXiv:2404.06904.
- Wang, C., Zheng, S., Zhong, L., Yu, C., Liang, C., Wang, Y., Gao, Y., Lam, T. L., & Shi, Y. (2024). PepperPose: Full-Body Pose Estimation with a Companion Robot. Proceedings of the CHI Conference on Human Factors in Computing Systems, 1–16.
- Wang, J., Wang, Y., Peng, L., Zhang, H., Gao, H., Wang, C., Gao, Y., Luo, H., & Chen, Y. (2024). Transformable Inspection Robot Design and Implementation for Complex Pipeline Environment. IEEE Robotics and Automation Letters.
- Chen, J., Gao, Y., Hu, J., Deng, F., & Lam, T. L. (2024). Meta-Reinforcement Learning Based Cooperative Surface Inspection of 3D Uncertain Structures using Multi-robot Systems. 2024 IEEE International Conference on Robotics and Automation (ICRA), 7201–7207. IEEE.

**2023:**

- Hu, J., Fan, C., Jiang, H., Guo, X., Gao, Y., Lu, X., & Lam, T. L. (2023). Boosting lightweight depth estimation via knowledge distillation. International Conference on Knowledge Science, Engineering and Management, 27–39. Springer Nature Switzerland Cham.
- Gao, Y., Chen, J., Chen, X., Wang, C., Hu, J., Deng, F., & Lam, T. L. (2023). Asymmetric self-play-enabled intelligent heterogeneous multirobot catching system using deep multiagent reinforcement learning. IEEE Transactions on Robotics, 39(4), 2603–2622.
- Wang, C., Gao, Y., Fan, C., Hu, J., Lam, T. L., Lane, N. D., & Bianchi-Berthouze, N. (2023). Learn2agree: Fitting with multiple annotators without objective ground truth. International Workshop on Trustworthy Machine Learning for Healthcare, 147–162. Springer Nature Switzerland Cham.
- Wang, Y., Lin, M., Xie, X., Gao, Y., Deng, F., & Lam, T. L. (2023). Asymptotically efficient estimator for range-based robot relative localization. IEEE/ASME Transactions on Mechatronics, 28(6), 3525–3536.
- Zhang, H., Luo, J., Gao, Y., & Ma, W. (2023). An intention inference method for the space non-cooperative target based on BiGRU-Self Attention. Advances in Space Research, 72(5), 1815–1828.
- Gao, Y., Fan, C., Hu, J., Lam, T. L., Lane, N. D., & Bianchi-Berthouze, N. (2023). Learn2Agree: Fitting with Multiple Annotators Without Objective Ground. Trustworthy Machine Learning for Healthcare: First International Workshop, TML4H 2023, Virtual Event, May 4, 2023, Proceedings, 13932, 147. Springer Nature.

**2022:**

- Hu, J., Fan, C., Ozay, M., Feng, H., Gao, Y., & Lam, T. L. (2022). Progressive self-distillation for ground-to-aerial perception knowledge transfer. arXiv Preprint arXiv:2208.13404.
- Gao, Y., Zhang, R., & Wang, H. (2022). On the asymptotic properties of a bagging estimator with a massive dataset. Stat, 11(1), e485.
- Wang, X., Zhang, W., Wang, C., Gao, Y., & Liu, M. (2023). Dynamic dense graph convolutional network for skeleton-based human motion prediction. IEEE Transactions on Image Processing, 33, 1–15.

**2021:**

- Yang, F., Gao, Y., Ma, R., Zojaji, S., Castellano, G., & Peters, C. (2021). A dataset of human and robot approach behaviors into small free-standing conversational groups. PloS One, 16(2), e0247364.
- Wang, C., Gao, Y., Mathur, A., De C. Williams, A. C., Lane, N. D., & Bianchi-Berthouze, N. (2021). Leveraging activity recognition to enable protective behavior detection in continuous data. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 5(2), 1–27.
- Deng, F., Feng, H., Liang, M., Feng, Q., Yi, N., Yang, Y., Gao, Y., & Lam, T. L. (2022). Abnormal occupancy grid map recognition using attention network. 2022 International Conference on Robotics and Automation (ICRA), 8666–8672. IEEE.
- Ahmad, M. I., Gao, Y., Alnajjar, F., Shahid, S., & Mubin, O. (2022). Emotion and memory model for social robots: a reinforcement learning based behaviour selection. Behaviour & Information Technology, 41(15), 3210–3236.
- Guan, H., Gao, Y., Zhao, M., Yang, Y., Deng, F., & Lam, T. L. (2021). AB-Mapper: Attention and BicNet Based Multi-agent Path Finding for Dynamic Crowded Environment. arXiv Preprint arXiv:2110.00760.
- Wang, C., Gao, Y., Fan, C., Hu, J., Lam, T. L., Lane, N. D., & Bianchi-Berthouze, N. (2021). Agreementlearning: An end-to-end framework for learning with multiple annotators without groundtruth. arXiv Preprint arXiv:2109.03596.

**2020:**

- Peng, M., Wang, C., Gao, Y., Bi, T., Chen, T., Shi, Y., & Zhou, X.-D. (2020). Recognizing micro-expression in video clip with adaptive key-frame mining. arXiv Preprint arXiv:2009.09179.
- Chen, X., Gao, Y., Ghadirzadeh, A., Bjorkman, M., Castellano, G., & Jensfelt, P. (2020). Skew-explore: Learn faster in continuous spaces with sparse rewards.
- Li, Chengxi, Castellano, G., & Gao, Y. (2020). Efficient Learning of Socially Aware Robot Approaching Behavior Toward Groups via Meta-Reinforcement Learning. IEEE/RSJ International Conference on Intelligent Robots and Systems, 12156–12159.
- Gao, Y. (2020). Machine Behavior Development and Analysis using Reinforcement Learning. Acta Universitatis Upsaliensis.

**2019:**

- Gao, Y., Yang, F., Frisk, M., Hemandez, D., Peters, C., & Castellano, G. (2019). Learning socially appropriate robot approaching behavior toward groups using deep reinforcement learning. 2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN), 1–8. IEEE.
- Gao, Y., Sibirtseva, E., Castellano, G., & Kragic, D. (2019). Fast adaptation with meta-reinforcement learning for trust modelling in human-robot interaction. 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 305–312. IEEE.
- Hernandez, D., Denamganaï, K., Gao, Y., York, P., Devlin, S., Samothrakis, S., & Walker, J. A. (2019). A generalized framework for self-play training. 2019 IEEE Conference on Games (CoG), 1–8. IEEE.

**2018:**

- Gao, Y., Wallkötter,S., Mohammad, O., & Castellano, G. (2018). Humanrobot proxemics using recurrent neural networks. IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN).
- Gao, Y., Wallkötter, S., Obaid, M., & Castellano, G. (2018). Investigating deep learning approaches for human-robot proxemics. 2018 27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), 1093–1098. IEEE.
- Gao, Y., Barendregt, W., Obaid, M., & Castellano, G. (2018). When robot personalisation does not help: Insights from a robot-supported learning study. 2018 27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), 705–712. IEEE.
- Li, Chengjie, Androulakaki, T., Gao, A. Y., Yang, F., Saikia, H., Peters, C., & Skantze, G. (2018). Effects of posture and embodiment on social distance in human-agent interaction in mixed reality. Proceedings of the 18th International Conference on Intelligent Virtual Agents, 191–196.

2017:
- Gao, A. Y., Barendregt, W., & Castellano, G. (2017). Personalised human-robot co-adaptation in instructional settings using reinforcement learning. IVA Workshop on Persuasive Embodied Agents for Behavior Change: PEACH 2017, August 27, Stockholm, Sweden.
- Zhang, P., Gao, A. Y., & Theel, O. (2017). Less is more: Learning more with concurrent transmissions for energy-efficient flooding. Proceedings of the 14th EAI International Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services, 323–332.

2016:
- Gao, Y., & Glowacka, D. (2016). Deep gate recurrent neural network. Asian Conference on Machine Learning, 350–365. PMLR.
2015:
- Gao, Y., Ilves, K., & Głowacka, D. (2015). Officehours: A system for student supervisor matching through reinforcement learning. Companion Proceedings of the 20th International Conference on Intelligent User Interfaces, 29–32.
<!--
<div class='container'>
    <div>
		<img src='papers/social_behavior_learning.gif' class='iconDetails'>
    </div>  
    <div style='margin-left:25%;'>
    <h4>Social Behavior Learning with Realistic Reward Shaping</h4>
    <div style="font-size:.8em"> Yuan Gao, Fangkai Yang, Martin Frisk, Daniel Hernandez, Christopher Peters and Ginevra Castellano</div>
	<h6></h6>
	<button class="button black" onclick="window.open('https://github.com/usr-lab/PepperSocial')" type="button">GitHub</button><button id="abstract_btn" class="button black" onclick="toggleAbstract(this);" type="button">Abstract</button><button class="button black" onclick="window.open('https://arxiv.org/pdf/1810.06979')" type="button">Download</button>
	<div id="abstract" text="Deep reinforcement learning has been widely applied in the field of robotics recently to study tasks like locomotion and grasping, but applying it to social robotics remains a challenge. In this paper, we present a deep learning scheme that acquires a prior model of robot behavior in a simulator as a first phase to be further refined through learning from subsequent real-world interactions involving physical robots. The scheme, which we refer to as Staged Social Behavior Learning (SSBL), considers different stages of learning in social scenarios. Based on this scheme, we implement robot approaching behaviors towards a small group generated from F-formation and evaluate the performance of different configurations using objective and subjective measures. We found that our model generates more socially-considerate behavior compared to a state-of-the-art model, i.e. social force model. We also suggest that SSBL could be applied to a wide class of social robotics applications."></div>
	<div style="float:right;font-size:.6em">arXiv, 2018</div>
    </div>
</div>


<div class='container'>
    <div>
		<img src='papers/effect_posture.jpg' class='iconDetails'>
    </div>  
    <div style='margin-left:25%;'>
    <h4>Effects of Posture and Embodiment on Social Distance in Human-Agent Interaction in Mixed Reality</h4>
    <div style="font-size:.8em"> Chengjie Li, Theofronia Androulakaki, Yuan Gao, Fangkai Yang, Himangshu Saikia, Christopher Peters and Gabriel Skantze</div>
	<h6></h6>
	<button id="abstract_btn" class="button black" onclick="toggleAbstract(this);" type="button">Abstract</button>
	<button class="button black" onclick="alert('Sorry, this paper is not publically available yet.')" type="button">Download</button>
	<div id="abstract" text="Mixed reality offers new potentials for social interaction experiences with virtual agents. In addition, it can be used to experiment with the design of physical robots. However, while previous studies have investigated comfortable social distances between humans and artificial agents in real and virtual environments, there is little data with regards to mixed reality environments. In this paper, we conducted an experiment in which participants were asked to walk up to an agent to ask a question, in order to investigate the social distances maintained, as well as the subject's experience of the interaction. We manipulated both the embodiment of the agent (robot vs. human and virtual vs. physical) as well as closed vs. open posture of the agent. The virtual agent was displayed using a mixed reality headset. Our experiment involved 35 participants in a within-subject design. We show that, in the context of social interactions, mixed reality fares well against physical environments, and robots fare well against humans, barring a few technical challenges."></div>
	<div style="float:right;font-size:.6em">Intelligent Virtual Agents, October 2018</div>
    </div>
</div>

<div class='container'>
    <div>
		<img src='papers/deep_proximics.jpg' class='iconDetails'>
    </div>  
    <div style='margin-left:25%;'>
    <h4>Human-Robot Proxemics using Recurrent Neural Networks</h4>
    <div style="font-size:.8em"> Yuan Gao, Sebastian Wallkötter, Mohammad Obaid and Ginevra Castellano</div>
	<h6></h6>
	<button id="abstract_btn" class="button black" onclick="toggleAbstract(this);" type="button">Abstract</button><button class="button black" onclick="window.open('papers/investigate-deep-learning-proximics.pdf')" type="button">Download</button>
	<div id="abstract" text="In this paper, we investigate the applicability of deep learning methods to adapt and predict comfortable human-robot proxemics. Proposing a network architecture, we experiment with three different layer configurations, obtaining three different end-to-end trainable models. Using these, we compare their predictive performances on data obtained during a human-robot interaction study. We find that our long short-term memory based model outperforms a gated recurrent unit based model and a feed-forward model. Further, we demonstrate how the created model can be exploited to create customized comfort zones that can help create a personalized experience for individual users. "></div>
    <div style="float:right;font-size:.6em">RO-MAN, 2018</div>
    </div>
</div>

<div class='container'>
    <div>
		<img src='papers/when_help.jpg' class='iconDetails'>
    </div>  
    <div style='margin-left:25%;'>
    <h4>When robot personalisation does not help: Insights from a robot-supported learning study </h4>
    <div style="font-size:.8em"> Yuan Gao, Wolmet Barendregt, Mohammad Obaid and Ginevra Castellano,</div>
    <h6></h6>
	<button id="abstract_btn" class="button black" onclick="toggleAbstract(this);" type="button">Abstract</button><button class="button black" onclick="window.open('papers/when-robot-does-not-help.pdf')" type="button">Download</button>
	<div id="abstract" text="In the domain of robotic tutors, personalised tutoring has started to receive scientists' attention, but is still relatively underexplored. Previous work using reinforcement learning (RL) has addressed personalised tutoring from the perspective of affective policy learning. However, little is known about the effects of robot behaviour personalisation on user's task performance. Moreover, it is also unclear if and when personalisation may be more beneficial than a robot that adapts to its users and the context of the interaction without personalising its behaviour. In this paper we build on previous work on affective policy learning that used RL to learn what robot's supportive behaviours are preferred by users in an educational scenario. We build a RL framework for personalisation that allows a robot to select verbal supportive behaviours to maximise the user's task progress and positive reactions in a learning scenario where a Pepper robot acts as a tutor and helps people to learn how to solve grid-based logic puzzles. 
A between-subjects design user study showed that participants were more efficient at solving logic puzzles and preferred a robot that exhibits more varied behaviours compared with a robot that personalises its behaviour by converging on a specific one over time. We discuss insights on negative effects of personalisation and report lessons learned together with design implications for personalised robots."></div>
    <div style="float:right;font-size:.6em">RO-MAN, 2018</div>
    </div>
</div>
-->
<br style/>
