<style>
a{color: white;}
</style>
    
    
#### 24th July, 2018 [Paper Accepted to [IVA](http://iva2018.westernsydney.edu.au/)]

>We conducted, to the best of our knowledge, one of the first studies comparing proxemics and other interpersonal factors with respect to varying embodiments of an agent such as type (human or robot), posture (open or closed) and virtuality (physical or virtual). We observed unexpected conclusions, such as participants remaining slightly further back on average for the open posture than the closed posture. We believe this has to do with the closed posture appearing to be in a troubled state, which attract concern from others.

<font size="1"> Chengjie Li, Theofronia Androulakaki, Yuan Gao, Fangkai Yang, Himangshu Saikia, Christopher Peters and Gabriel Skantze. <b>Effects of Posture and Embodiment on Social Distance in Human-Agent Interaction in Mixed Reality.</b> 18th ACM International Conference on Intelligent Virtual Agents, IVA, 2018. </font>

#### 13th July, 2018 [Attending [IJCAI](https://www.ijcai-18.org/)]

>I will be attending the 27th IJCAI at Stockholm, Sweden. Hope to see all old and new friends there. 

#### 10th June, 2018 [Paper Accepted to [ROMAN](http://ro-man2018.org/)]

>We investigate the applicability of deep learning methods to adapt and predict comfortable human-robot proxemics. Proposing a network architecture, we experiment with three different layer configurations, obtaining three different end-to-end trainable models. Using these, we compare their predictive performances on data obtained during a human-robot interaction study. We find that our long short-term memory based model outperforms a gated recurrent unit based model and a feed-forward model. Further, we demonstrate how the created model can be used to create customized comfort zones that can help create a personalized experience for individual users.

<font size="1"> Yuan Gao, Sebastian Wallk√∂tter, Mohammad Obaid, Ginevra Castellano, <b>Investigating Deep Learning Approaches for Human-Robot Proxemics</b>, IEEE International Conference on Robot and Human Interactive Communication (RO-MAN 2018), 2018.</font>

#### 10th June, 2018 [Paper Accepted to [ROMAN](http://ro-man2018.org/)]

>In the domain of robotic tutors, personalised tutoring has started to receive scientists' attention, but is still relatively underexplored.  We build a RL framework for personalisation that allows a robot to select verbal supportive behaviours to maximise the user's task progress and positive reactions in a learning scenario where a Pepper robot acts as a tutor and helps people to learn how to solve grid-based logic puzzles. A between-subjects design user study showed that participants were more efficient at solving logic puzzles and preferred a robot that exhibits more varied behaviours compared with a robot that personalises its behaviour by converging on a specific one over time. We discuss insights on negative effects of personalisation and report lessons learned together with design implications for personalised robots.

<font size="1"> Yuan Gao, Wolmet Barendregt, Mohammad Obaid, Ginevra Castellan, <b>When robot personalisation does not help: Insights from a robot-supported learning study</b>, IEEE International Conference on Robot and Human Interactive Communication (RO-MAN 2018), 2018.</font>
